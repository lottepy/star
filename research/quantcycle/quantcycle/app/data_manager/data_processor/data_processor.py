""" DataProcessor module

This module is under the DataManager module. 
This module aims to process data collected by DataLoader module according to the 
signals given by the DataLoader module, including construction of useful df and 
numpy array, and calculation of extra info that cannot be loaded from any data source. 

Major flow of data processing is under the 'run' method.

Major data processing "methods" are defined in the Method class under DataProcessor.
Methods will be called in _handle_data function.
METHOD_MAP stores all the available class names of methods.
To add new data process method:
    1. add a new key and value into METHOD_MAP
    2. construct a new class under 'method' folder under MethodBase class
    3. strictly refer to the structure of existing 'method' classes
    4. write unittest for new method   

Please refer to the README.md for details

Todo:
    * For module TODOs
"""
# from .method.interest_rate_calculator import calculate_interest_rate

from datetime import datetime, timedelta
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd

from quantcycle.app.data_manager.utils import DATA_MANAGER_LOG_ROOT, DATA_MANAGER_LOG_PARENT
from quantcycle.app.data_manager.utils.data_collection import update_data
from quantcycle.app.data_manager.utils.timestamp_manipulation import list2timearr
from quantcycle.utils.get_logger import get_logger
from quantcycle.app.data_manager.data_loader.data_center.data.dm_constants import DATA_MASTER_LOAD_BATCH, LabelField

from .method import METHOD_MAP
from quantcycle.app.data_manager.utils.update_nested_dict import update_dict


class DataProcessor():
    """ To process data passed by DataLoader, construct dataframe, and eventually
    numpy array for DataDistributor.

    Attributes:
        data_collection (Dict[pd.DataFrame]): 
            Collection of data downloaded or accquired by DataLoader, or processed 
            by DataProcessor.
        data_manager (DataManager): 
            The ONLY obj generated by engine that can be used to call other methods
            or attributes under DataManager module.
        data_bundles: (Dict): 
            Data bundles generated by DataLoader. Include information on actions
            required to be completed by DataProcessor.
            e.g. dict 
            {'data_bundle["Label"]': ..., 'data_bundle["Label"]': ..., ...}
        dict_data_array (Dict[str, Tuple[pd.DataFrame, List, List]]):
            Stores DataFrame, symbols, fields info constructed by DataProcessor, 
            later transfers into numpy array, and is passed to DataDistributor for 
            conversion into raw array.
            e.g. dict of np array
            {'data_arr': ...., 'time_arr': ..., 'symbol_arr': ..., 'fields_arr': ...}
    """

    def __init__(self):
        self.data_collection = None
        self.data_manager = None
        self.data_bundles = []
        self.dict_data_array: Dict[str, Tuple[pd.DataFrame, List, List]] = {}
        self.dict_data_str: Dict = {}
        logger_name = DATA_MANAGER_LOG_PARENT+'.DataManager.DataProcessor' if DATA_MANAGER_LOG_PARENT else 'DataManager.DataProcessor'
        self.logger = get_logger(
            logger_name,
            str(DATA_MANAGER_LOG_ROOT.joinpath(
                f'{self.__class__.__name__}.log'))
        )

    def prepare(self, data_manager) -> None:
        ''' To provide a link for DataProcessor to call methods under DM

        Args:
            data_manager (DataManager): DataManager obj
        '''
        self.data_manager = data_manager

    def register(self, data_bundle: Dict) -> None:
        ''' Register signals from data_loader 

        Args:
            data_bundle (dict): dict of data info passed by DataLoader
        '''
        if data_bundle["Type"] == "Process":
            self.data_bundles.append(data_bundle)

    def run(self) -> None:
        ''' The major data processing sequences of DataProcessor '''
        # 1. generate all requested df
        self.logger.info(f'Run: Handling Data')
        self._handle_data()

        # 2. convert all df into numpy array
        self.logger.info(f'Run: Converting df to np')
        self._convert_df_to_np()    # df obj will be overwritten by dict obj in this function

        # 3. pass all np array pending for packaging into Raw Array
        self.logger.info(f'Run: Passing data to distributor')
        self._pass_data_to_distributor()

    def _convert_df_to_np(self) -> None:
        ''' To convert constructed df into numpy array for DD '''
        for key in self.dict_data_array.keys():
            # 1. convert all data df into numpy array
            df, symbol_list, fields_list = self.dict_data_array[key]
            if df.empty:
                dict_np = {'data_arr': np.empty((0,0,0)), 'time_arr': np.empty((0,0,0))}
            else:
                dict_np = _df_to_numpy(len(symbol_list), df)
            dict_np['symbol_arr'] = symbol_list
            dict_np['fields_arr'] = fields_list
            # 2 overwrite the tuple into a dict_data_array for distributor
            self.dict_data_array[key] = dict_np

    def _pass_data_to_distributor(self) -> None:
        ''' Pass processed data array to data_distributor '''
        self.data_manager.data_distributor_main.dict_np_data_need_pack.update(self.dict_data_array)
        self.data_manager.data_distributor_main.dict_str_data_need_pack.update(self.dict_data_str)

    def _connect_queue(self):
        raise NotImplementedError

    def _handle_data(self) -> None:
        ''' Handle data according to signals registered in data_bundles info.'''
        for data_bundle in self.data_bundles:
            self.logger.info(f'On data bundle: {data_bundle}')
            # determine what kind of action is needed
            actions = [data_bundle["Actions"]]
            if data_bundle["DataCenter"] == "DataManager":
                action_type = data_bundle["Actions"].lower()
                results = getattr(self, f'_{action_type}')(data_bundle)
                if data_bundle['DataType'] == 'string':
                    self.dict_data_str[data_bundle["Label"]] = results
                elif data_bundle['DataType'] == 'numpy':
                    self.dict_data_array[data_bundle["Label"]] = results
            else:
                for action in actions:
                    self.logger.info(f'Action: {action}')
                    m = METHOD_MAP[action]()
                    try:
                        m.on_bundle(data_bundle, self.data_collection)
                    except Exception as e:
                        self.logger.error(f'{e}')
                        raise e
                    if m.is_final_step:
                        if m.output_data_type == 'df':
                            if self.dict_data_array.get(data_bundle["Label"], None) and data_bundle["DataCenter"] == 'LocalCSV':
                                # for 'localCSV' append data
                                self.dict_data_array[data_bundle["Label"]] = m.run()
                            else:
                                self.dict_data_array[data_bundle["Label"]] = m.run()
                        elif m.output_data_type == 'bypass_arr2raw':
                            self.dict_data_str[data_bundle["Label"]] = m.run()
                        elif m.output_data_type == 'np_numeric':
                            self.data_manager.data_distributor_main.dict_np_data_need_pack.update({data_bundle["Label"]: m.run()})
                        else:
                            raise ValueError(
                                f'The output_data_type of method "{action}" is undefined.')
                    else:
                        update_data(self.data_collection, m.run())
    def _merge_info(self, data_bundle):
        res_info = {}
        keys_sub = data_bundle["Symbol"]
        [update_dict(res_info, self.dict_data_str[f'{key}/INFO']) for key in keys_sub]
        return res_info

    def _stack(self, data_bundle):
        '''
            Stack multi assets data including
            1. MAIN
            2. CCPFXRATE
            3. INT
        '''
        start_ts = data_bundle["StartTS"]
        keys_sub = data_bundle["Symbol"]
        field = data_bundle["Fields"][0]
        if field in ["MAIN", "CCPFXRATE", "INT"]:
            try:
                data = [self.dict_data_array[f'{key}/{field}'] for key in keys_sub]
            except Exception as e:
                self.logger.error(f'Data key not exist in DataManager during multi assets STACK: {e}')
                raise e
        else:
            data = [self.dict_data_array.get(f'{key}/{field}', ([],[],[])) for key in keys_sub]

        list_items = list(zip(*data))
        lst_dfs = list(list_items[0])
        lst_symbols = list(list_items[1])
        lst_fields = list(list_items[2])
        if field == "MAIN":
            self.asset_sym_lists = lst_symbols

        if field not in ["MAIN", "CCPFXRATE", "INT"]:
            for i, d in enumerate(lst_dfs):
                if len(d)==0:
                    lst_symbols[i] = self.asset_sym_lists[i]
                    lst_fields[i], lst_dfs[i] = _set_default_values(lst_symbols[i], field, start_ts)

        df = pd.concat(lst_dfs, axis=1)
        if field == 'INT':
            df = df.fillna(0.0)
        elif field == 'CCPFXRATE':
            df = df.ffill()
        symbols = [val for sublist in lst_symbols for val in sublist]
        fields = lst_fields[0]

        return df, symbols, fields

def _set_default_values(symbol_list, data_type, start_ts):
    if data_type == 'CONTRACTS_INFO':
        fields = ['individual_contract', 'next_close']
        df = pd.DataFrame(np.nan, columns=len(symbol_list)*fields, index=[start_ts])
        # df = df.fillna(0)
    elif data_type == 'SPLIT':
        fields = ['split_ratio_last']
        df = pd.DataFrame(np.nan, columns=len(symbol_list)*fields, index=[start_ts])
        df = df.fillna(1.0)
    return fields, df

# def _update_data_array_data(original_tuple: tuple, res_tuple: tuple, data_bundle) -> tuple:
#     '''
#         for 'localCSV' append data
#         Logic:
#         1. if symbol exists originally, overwrite the existing one
#         2. Data of new symbols will be appended sideway to the original dataframe

#     Args:
#         original_tuple (tuple): tuple of original data
#         res_tuple (tuple): tuple of new data from local csv

#     Returns: 
#         tuple: (original_tuple[0], df_symbols, original_tuple[2])
#     '''
#     dm_map = data_bundle.get('SymbolArgs',{}).get('dm_map',np.array([]))
#     symbols = data_bundle.get('Symbol', [])
#     label_field = data_bundle.get('Label', '').split('/')[-1]
#     if label_field not in LabelField.__members__:
#         return res_tuple
    
#     dm_map_lst = dm_map[LabelField[label_field].value]
#     ind_list = np.where(dm_map_lst==False)[0]
#     # # original_tuple[0].update(res_tuple[0], overwrite=True)
#     # original_symbols = list(original_tuple[1])
#     # # new_symbols = [sym for sym in res_tuple[1] if sym not in original_symbols]    
#     # new_symbols = res_tuple[1]
#     # sym_map = {v:i for i,v in enumerate(original_symbols)}
#     # new_sym_ind = [sym_map[sym] for sym in new_symbols]
#     #[for i in ]
#     old_df = original_tuple[0]
#     new_df = res_tuple[0]

            

#     num_fields = len(original_tuple[2])

   
#     # start = 0
#     # df_list = []
#     # for i in ind_list:
#     #     if i != start:
#     #         df_list.append(old_df.iloc[:,start:i*num_fields])
#     #         df_list.append(new_df.iloc[:,i])
#     #     else:
#     #         pass
#     # new_df = pd.concat(df_list, axis=1, join='inner')
    
#     new_df = new_df[original_symbols]
#     return (new_df,original_symbols,original_tuple[2])

def _df_to_numpy(num_symbols: int, data_df: pd.DataFrame) -> dict:
    ''' Convert df into 3D array

    Args:
        num_symbols (int): Total number of symbols
        data_df (pd.DataFrame): Df to be converted into numpy array

    Returns: 
        dict: {'data_arr': np.ndarray, 'time_arr': np.ndarray}
    '''

    data_arr = data_df.values.reshape(data_df.shape[0], num_symbols, -1)

    time_arr = list2timearr(data_df.index)

    return {'data_arr': data_arr, 'time_arr': time_arr}